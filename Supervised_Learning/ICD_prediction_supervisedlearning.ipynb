{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "Extracting codes from standard terminologies is a regular and indispensable task often encountered in medical and healthcare fields. Diagnosis codes, procedure codes, cancer site and morphology codes are all manually extracted from patient records by trained human coders. There are servrals use cases of extracted codes, for example: \n",
    "- Insurance billing and reimbursement \n",
    "- Medical facilities quality control \n",
    "- Scientific work such as Epidemiological studies \n",
    "- Cohort identification for clinical trials. \n",
    "\n",
    "In this repoort using combination of unsupervised and supervised machine learning we will try to extracting international classification of diseases, clinical modification, 9th revision (ICD-9-CM) diagnosis codes from electronic health records (EHRs). \n",
    "\n",
    "The dataset contains de-identified EHRs containing both diagnostic and procedural ICD codes, but we will focus on extracting only the most common diagnostic ICD codes. The methods and approach used in this report are general and it can also be applied to other medical code extraction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore the supervised machine learning portion.\n",
    "##### Motivation\n",
    "\n",
    "Our goal for supervised machine learning portion is to process features extracted from the relevant portion of the patient's Electronic Health Record (EHR) and predict relevant diagnostic ICD codes related to the EHR diagnosis. As more than one code can be assigned to the same EHR diagnosis, the supervised learning approach to use is Multi-Label Classification, where each ICD code is treated as individual label. There are many real life examples of Multi Label classifications, such categorizing movies by genere, classifying news articles, labeling objects from image classifications, etc. \n",
    "\n",
    "Multi Label classification probel can be often confused with Multi Class classification problem. Multi Class classification assumes that classes are mutually exclusive. Multi label classification treats each label as a different classification problem. What we have here is a binary Multi Label problem, where we try to identify if each label i.e. the ICD-9 code is appropriate or not appropriate for the EHR diagnostic summary of the patient. \n",
    "We wish to present this as a solution to aid the human medical coder in assigning correct ICDs as a decision support application. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, time\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score, hamming_loss, roc_auc_score \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "import xgboost as xgb\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Location Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used MIMIC-III dataset for training and validating our models. MIMIC-III is an open dataset of medical records of ICU stays of deidentified patients at Beth-Israel Hospital. It has EHRs of 40000 individuals, data contained in it is deidentified. More information about the data and how to access it is given in the initial Data section of the report which is common for both unsupervised and supervised learning. \n",
    "\n",
    "Here we will only discuss about the data manipulation and relevant information extraction which is relevant for the supervised portion of the report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of your data directory\n",
    "datadir = \"Data/\"\n",
    "#Location of your model directory\n",
    "mdldir = \"Models/\"\n",
    "# Make sure to have following files in the data directory:\n",
    "# 1. DIAGNOSES_ICD.csv : Assigned diagnostic ICDs for each EHR\n",
    "diagicd_file = \"DIAGNOSES_ICD.csv\"\n",
    "# 2. df_cleaned_wt_prob_topic_dist.pkl : Preprocessed DataFrame\n",
    "dfpkl_file = \"df_cleaned_wt_prob_topic_dist.pkl\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag_token_extract(text):\n",
    "    # Function takes string input of EHR text\n",
    "    # Extracts \"Discharge Diagnosis\" portion of the full note\n",
    "    # and returns extracted word tokens\n",
    "    rawdiags = re.findall(r\"(?<=Diagnos.s:\\n)((.+\\n)*)(?=)\", text)\n",
    "    rawdiags = [x[0].split('\\n')[:-1] for x in rawdiags]\n",
    "    findiags = [diag.lower().split() for diags in rawdiags for diag in diags if diag != 'Primary Diagnosis:']\n",
    "    fltdiags = [diag for sublist in findiags for diag in sublist]\n",
    "    alfdiags = [item for item in fltdiags if item.isalpha()]\n",
    "    return alfdiags\n",
    "\n",
    "#_____________________________________________________________\n",
    "# Insert missing \".\" in ICD codes\n",
    "def icd_dot(initcode, isdiagcode=False):\n",
    "    # Function reformats ICD codes present in the raw data\n",
    "    # from abcxy format to abc.xyz format\n",
    "    initcode = list(str(initcode))\n",
    "    icdcode = initcode[:]\n",
    "    if isdiagcode==False:\n",
    "        icdcode.insert(2,'.')\n",
    "    else:\n",
    "        if initcode[0]=='E':\n",
    "            if len(initcode)>4:\n",
    "                icdcode.insert(4,'.')\n",
    "        else:\n",
    "            if len(initcode)>3:\n",
    "                icdcode.insert(3,'.')\n",
    "        icdcode=icdcode[:5] # Regroup ICDs at abc.x level by stripping yz codes sections\n",
    "    return \"\".join(icdcode)\n",
    "\n",
    "#_____________________________________________________________\n",
    "def onelevelup(codes,topicds):\n",
    "    # Function takes string consisting of ICD 9 codes (abc.xyz format)\n",
    "    # Reformats them to abc.x format to improve avg.codes/document density\n",
    "    # Outputs a set of reformatted codes\n",
    "    codes = codes.split(\",\")\n",
    "    setofcodes = {''.join(list(code)[:5]) for code in codes if code in topicds}\n",
    "    return list(setofcodes)#', '.join(setofcodes)\n",
    "#_____________________________________________________________\n",
    "def dataprep(df, diagicd_df, mostcommon=50):\n",
    "    # Use only if unsupervised Topic Modeling exercise is not done \n",
    "    # and topic vectors are not present in the saved pickle file\n",
    "    # Restructuring ICD code format \n",
    "    diagicd_df['ICD_CODE']=diagicd_df['ICD9_CODE'].apply(lambda x: icd_dot(x,isdiagcode=True))\n",
    "    topicds=Counter(diagicd_df.ICD_CODE).most_common(mostcommon)\n",
    "    topicds = [icd[0] for icd in topicds] # Most common ICDs in descending order as list\n",
    "    df['ICDset'] = df['ICDs'].apply(lambda x: onelevelup(x, topicds)) # Format ICD to abc.x format\n",
    "    df['Tokens'] = df['TEXT'].apply(lambda x: diag_token_extract(x)) # Extract word tokens\n",
    "\n",
    "    df = df[df['Tokens'].map(lambda d: len(d)) > 0] # Drop rows with no tokens\n",
    "    df = df[df['ICDset'].map(lambda d: len(d)) > 0] # Drop rows with no labels i.e. ICD codes\n",
    "\n",
    "    print(\"Shape of the dataset = \",df.shape)\n",
    "    print(\"Number of Unique set of ICDs = \", df['ICDset'].astype(str).nunique())\n",
    "\n",
    "    # Transforming prediction target 'y' to multilabel matrix 'y_'\n",
    "    y = df['ICDset']\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_ = mlb.fit_transform(y)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df['Tokens'],\n",
    "                                              y_, test_size=0.2)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "#_____________________________________________________________\n",
    "def dataprep_topmdlfeat(df, diagicd_df, mostcommon=50, numtopics=20):\n",
    "    # Function\n",
    "    # Restructuring ICD code format \n",
    "    diagicd_df['ICD_CODE']=diagicd_df['ICD9_CODE'].apply(lambda x: icd_dot(x,isdiagcode=True))\n",
    "    topicds=Counter(diagicd_df.ICD_CODE).most_common(mostcommon)\n",
    "    topicds = [icd[0] for icd in topicds] # Most common ICDs in descending order as list\n",
    "    df['ICDset'] = df['ICDs'].apply(lambda x: onelevelup(x, topicds)) # Format ICD to abc.x format\n",
    "    df['Tokens'] = df['TEXT'].apply(lambda x: diag_token_extract(x)) # Extract word tokens\n",
    "\n",
    "    df = df[df['Tokens'].map(lambda d: len(d)) > 0] # Drop rows with no tokens\n",
    "    df = df[df['ICDset'].map(lambda d: len(d)) > 0] # Drop rows with no labels i.e. ICD codes\n",
    "\n",
    "    print(\"Shape of the dataset = \",df.shape)\n",
    "    print(\"Number of Unique set of ICDs = \", df['ICDset'].astype(str).nunique())\n",
    "\n",
    "    # Expanding topic model distribution vector to columns\n",
    "    global topiclist\n",
    "    topiclist = ['topic_'+ str(i) for i in range(numtopics)]\n",
    "    df[topiclist] = pd.DataFrame(df.prob_dist_topics.tolist(), index= df.index)\n",
    "    \n",
    "    # Transforming prediction target 'y' to multilabel matrix 'y_'\n",
    "    y = df['ICDset']\n",
    "    global mlb\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_ = mlb.fit_transform(y)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[topiclist+['Tokens']],\n",
    "                                              y_, test_size=0.2)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "#______________________________________________________________________\n",
    "def humaneval(y_test,y_pred,id=111):\n",
    "    print(\"Human assigned ICDs - \", mlb.inverse_transform(y_test)[id])\n",
    "    print(\"Model assigned ICDs - \",mlb.inverse_transform(y_pred)[id])\n",
    "    print(\"Correctly assigned ICDs - \",[i for i in mlb.inverse_transform(y_pred)[id] if i in mlb.inverse_transform(y_test)[id]])\n",
    "    print('Features extracted from Discharge Diagnosis: ', X_test['Tokens'].iloc[id])\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared dataframe\n",
    "df = pd.read_pickle(datadir+dfpkl_file)\n",
    "diagicd_df = pd.read_csv(datadir+diagicd_file)\n",
    "# Loading mdlresults to keep track of models and \n",
    "try:\n",
    "    mdlresults_df.head(1)\n",
    "except: \n",
    "    mdlresults_df = pd.read_pickle(mdldir+'mdlresults_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset =  (36783, 9)\n",
      "Number of Unique set of ICDs =  26618\n"
     ]
    }
   ],
   "source": [
    "# Split data into Training and Test sets\n",
    "mc, num = 100, 20 # Dataset for #'mc' most common ICDs, num = number of topics from topic modeling\n",
    "X_train, X_test, y_train, y_test = dataprep_topmdlfeat(df, diagicd_df, mostcommon=mc, numtopics=num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: Methods, Hyperparameters and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods - \n",
    "Most of the machine learning classifiers are built for single target classification problems i.e. for a  single label with binary class values. So we transformed a multi label classification problem to multiple single label problems using the following techniques. ScikitLearn’s One-Vs-Rest classifier and MultiOutputClassifier. In our observations both of the classifiers performed similarly and hence we decided to go with One-Vs-Rest classifier for our final model.\n",
    "\n",
    "Once, a multi label classification problem is transformed to multiple single label classification problem, we experimented with following classifiers for single label classification -  \n",
    "- Logistic Regression \n",
    "- Support Vector Machines (SVM) \n",
    "- XGBoost \n",
    "(Note: In XGBoost 1.6, it has experimental support for multi-output regression and multi-label classification with Python package. More details for the same can be found here. [[https://xgboost.readthedocs.io/en/stable/tutorials/multioutput.html]].) \n",
    "Feature Tuning: \n",
    "\n",
    "We then trained models using TF-IDF word vectors as features with and without topic vectors obtained from topic modeling done in the unsupervised learning section.\n",
    "We also experimented with following features and compared different models’ performance - \n",
    "\n",
    "For tf-idf vectorizer:  \n",
    "max_df : We experimented with max_df in the range of .95 to 0.75 to ignore words higher than the set threshold.  \n",
    "Max_features : Top 1000, 10000, 25000 and 50000 word tokens ordered by term frequency  \n",
    "  \n",
    "For classifiers:  \n",
    "Solver: We tried different solvers for the Logistic Regression classifier, and picked ‘sag’ solver for our final model  \n",
    "max_iter : We experimented with 100 (default), 1000, 2500, and 5000 max_iter limits to help the solver converge on a solution. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF helper function for pipeline\n",
    "def identity_tokenizer(text):\n",
    "        return text\n",
    "\n",
    "def model_pipeline_evals(max_df = 0.8, max_features = 50000, max_iter=2500, estimator_ind=0, classifier_ind=0, topicvec = 0):\n",
    "    # Function to pick estimator, classifier and topicvector as additional feature\n",
    "    # estimator_ind, classifier_ind = 0, 0 #default vals\n",
    "    # Function outputs, predicted labels for test dataset and the supervised learning model\n",
    "\n",
    "    tfidf_vec = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False, \n",
    "            max_df=max_df, max_features=max_features)\n",
    "\n",
    "    # Model selection\n",
    "    estimators_dict = {\n",
    "                        'lr':LogisticRegression(max_iter=max_iter, solver='sag', multi_class='ovr'), #, class_weight='balanced'\n",
    "                        'svm': LinearSVC(max_iter=max_iter, penalty='l2'),\n",
    "                        'xgb': xgb.XGBClassifier()\n",
    "                    }\n",
    "    estimators_list = list(estimators_dict.keys())\n",
    "    estimator = estimators_dict[estimators_list[estimator_ind]] \n",
    "    # estimator = estimators_dict[estimators_list[estimator_ind]] #default\n",
    "\n",
    "    # Multilabel classification have to be converted to a binary problem. \n",
    "    # The two most common conversions are:\n",
    "    #           OvO - One vs One\n",
    "    #           OvR - One vs Rest  \n",
    "    classifier_dict = {\n",
    "                        'ovr': OneVsRestClassifier(estimator),\n",
    "                        'moc': MultiOutputClassifier(estimator, n_jobs=-1),\n",
    "                        'xgb': xgb.XGBClassifier(tree_method=\"hist\")\n",
    "                    }\n",
    "    classifier_list = list(classifier_dict.keys())\n",
    "    classifier = classifier_dict[classifier_list[classifier_ind]]\n",
    "\n",
    "    if topicvec == 0:\n",
    "        # Pipeline for supervised learning without Topic Vectors\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                (\"tfidf\", tfidf_vec),\n",
    "                (\"clf\", classifier), \n",
    "            ]\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        mdl = pipeline.fit(X_train['Tokens'], y_train)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        y_pred = mdl.predict(X_test['Tokens'])\n",
    "\n",
    "    else:\n",
    "    # Pipeline for supervised learning with Topic Vectors\n",
    "        preprocess = ColumnTransformer([\n",
    "                ('distribution_norm', StandardScaler(), topiclist),\n",
    "                (\"tfidf\", tfidf_vec, \"Tokens\")\n",
    "            ],remainder=\"passthrough\"\n",
    "        )\n",
    "\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                (\"preprocess\", preprocess),\n",
    "                (\"clf\", classifier),\n",
    "            ]\n",
    "        )\n",
    "        start_time = time.time() \n",
    "        mdl = pipeline.fit(X_train, y_train) # mdl fit\n",
    "        elapsed_time = time.time() - start_time\n",
    "        y_pred = mdl.predict(X_test)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(\n",
    "        y_test.ravel(), y_pred.ravel()\n",
    "    )\n",
    "    mdlname = estimators_list[estimator_ind]+'_'+classifier_list[classifier_ind]+'_'+str(mc)+'_'+str(topicvec)+'.pkl'\n",
    "    joblib.dump(mdl, mdldir+mdlname) #pickle.dump(mdl, open(mdldir+mdlname, 'wb'))\n",
    "    print(\"Trained Model :\", mdlname, \"Training time :\",'{0:.2f}'.format(elapsed_time))\n",
    "\n",
    "    score = [\n",
    "                mdlname,\n",
    "                topicvec,\n",
    "                f1_score(y_test, y_pred, average=\"micro\"),\n",
    "                accuracy_score(y_test, y_pred),\n",
    "                average_precision_score(y_test, y_pred, average=\"micro\"),\n",
    "                roc_auc_score(y_test, y_pred, average=\"micro\"),\n",
    "                hamming_loss(y_test, y_pred),\n",
    "                '{0:.2f}'.format(elapsed_time),\n",
    "                precision,\n",
    "                recall\n",
    "            ]\n",
    "    mdlresults_df.loc[len(mdlresults_df.index)] = score\n",
    "    return y_pred, mdl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Evaluation Metrics Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The code below will train the models to predict user chosen number for most common ICDs in the dataset, and save the models as pickle file and save the performance evaluation dataframe also as a pickle file in the Models folder. The performance of model is evaluated over 5 randomly generated sets of Training and Test data, and we will finally see the average performance of each model in the resulting dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Uncomment the code below if you wish to retrain the models and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mc in [50, 100, 250]:\n",
    "#     num = 20\n",
    "#     for _ in range(5): # Repeat 5 times to get average of evaluation metrics\n",
    "#         X_train, X_test, y_train, y_test = dataprep_topmdlfeat(df, diagicd_df, mostcommon=mc, numtopics=num)\n",
    "#         for i in range(3):\n",
    "#             for j in range(2):\n",
    "#                 model_pipeline_evals(max_df=0.8, max_features=50000, max_iter=2500, estimator_ind=i, classifier_ind=0, topicvec = j)\n",
    "# mdlresults_df.to_pickle(\"mdlresults_df\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logging history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"###### Logging history:\n",
    "Shape of the dataset =  (34774, 9)\n",
    "Number of Unique set of ICDs =  9616\n",
    "Trained Model : lr_ovr_50_0.pkl Training time : 4.59\n",
    "Trained Model : lr_ovr_50_1.pkl Training time : 11.98\n",
    "Trained Model : svm_ovr_50_0.pkl Training time : 1.57\n",
    "Trained Model : svm_ovr_50_1.pkl Training time : 93.89\n",
    "Trained Model : xgb_ovr_50_0.pkl Training time : 45.29\n",
    "Trained Model : xgb_ovr_50_1.pkl Training time : 90.90\n",
    "Shape of the dataset =  (34774, 9)\n",
    "Number of Unique set of ICDs =  9616\n",
    "Trained Model : lr_ovr_50_0.pkl Training time : 4.32\n",
    "Trained Model : lr_ovr_50_1.pkl Training time : 12.43\n",
    "Trained Model : svm_ovr_50_0.pkl Training time : 4.25\n",
    "Trained Model : svm_ovr_50_1.pkl Training time : 95.44\n",
    "Trained Model : xgb_ovr_50_0.pkl Training time : 49.63\n",
    "Trained Model : xgb_ovr_50_1.pkl Training time : 90.97\n",
    "Shape of the dataset =  (34774, 9)\n",
    "Number of Unique set of ICDs =  9616\n",
    "Trained Model : lr_ovr_50_0.pkl Training time : 4.54\n",
    "Trained Model : lr_ovr_50_1.pkl Training time : 12.81\n",
    "Trained Model : svm_ovr_50_0.pkl Training time : 1.64\n",
    "Trained Model : svm_ovr_50_1.pkl Training time : 87.48\n",
    "Trained Model : xgb_ovr_50_0.pkl Training time : 44.16\n",
    "Trained Model : xgb_ovr_50_1.pkl Training time : 90.86\n",
    "Shape of the dataset =  (34774, 9)\n",
    "Number of Unique set of ICDs =  9616\n",
    "Trained Model : lr_ovr_50_0.pkl Training time : 5.49\n",
    "Trained Model : lr_ovr_50_1.pkl Training time : 12.87\n",
    "Trained Model : svm_ovr_50_0.pkl Training time : 1.55\n",
    "Trained Model : svm_ovr_50_1.pkl Training time : 94.03\n",
    "Trained Model : xgb_ovr_50_0.pkl Training time : 47.13\n",
    "Trained Model : xgb_ovr_50_1.pkl Training time : 95.25\n",
    "Shape of the dataset =  (34774, 9)\n",
    "Number of Unique set of ICDs =  9616\n",
    "Trained Model : lr_ovr_50_0.pkl Training time : 4.81\n",
    "Trained Model : lr_ovr_50_1.pkl Training time : 13.61\n",
    "Trained Model : svm_ovr_50_0.pkl Training time : 1.56\n",
    "Trained Model : svm_ovr_50_1.pkl Training time : 101.40\n",
    "Trained Model : xgb_ovr_50_0.pkl Training time : 46.76\n",
    "Trained Model : xgb_ovr_50_1.pkl Training time : 93.99\n",
    "Shape of the dataset =  (35837, 9)\n",
    "Number of Unique set of ICDs =  19547\n",
    "Trained Model : lr_ovr_100_0.pkl Training time : 8.30\n",
    "Trained Model : lr_ovr_100_1.pkl Training time : 25.71\n",
    "Trained Model : svm_ovr_100_0.pkl Training time : 3.87\n",
    "Trained Model : svm_ovr_100_1.pkl Training time : 143.56\n",
    "Trained Model : xgb_ovr_100_0.pkl Training time : 94.78\n",
    "Trained Model : xgb_ovr_100_1.pkl Training time : 194.13\n",
    "Shape of the dataset =  (35837, 9)\n",
    "Number of Unique set of ICDs =  19547\n",
    "Trained Model : xgb_ovr_100_1.pkl Training time : 193.96\n",
    "Shape of the dataset =  (35837, 9)\n",
    "Number of Unique set of ICDs =  19547\n",
    "Trained Model : lr_ovr_100_0.pkl Training time : 8.64\n",
    "Trained Model : lr_ovr_100_1.pkl Training time : 23.41\n",
    "Trained Model : svm_ovr_100_0.pkl Training time : 2.68\n",
    "Trained Model : svm_ovr_100_1.pkl Training time : 131.33\n",
    "Trained Model : xgb_ovr_100_0.pkl Training time : 88.41\n",
    "Trained Model : xgb_ovr_100_1.pkl Training time : 205.23\n",
    "Shape of the dataset =  (35837, 9)\n",
    "Number of Unique set of ICDs =  19547\n",
    "Trained Model : lr_ovr_100_0.pkl Training time : 8.06\n",
    "Trained Model : lr_ovr_100_1.pkl Training time : 24.45\n",
    "Trained Model : svm_ovr_100_0.pkl Training time : 2.74\n",
    "Trained Model : svm_ovr_100_1.pkl Training time : 147.86\n",
    "Trained Model : xgb_ovr_100_0.pkl Training time : 90.81\n",
    "Trained Model : xgb_ovr_100_1.pkl Training time : 213.17\n",
    "Shape of the dataset =  (35837, 9)\n",
    "Number of Unique set of ICDs =  19547\n",
    "Trained Model : lr_ovr_100_0.pkl Training time : 8.64\n",
    "Trained Model : lr_ovr_100_1.pkl Training time : 23.41\n",
    "Trained Model : svm_ovr_100_0.pkl Training time : 2.68\n",
    "Trained Model : svm_ovr_100_1.pkl Training time : 131.33\n",
    "Trained Model : xgb_ovr_100_0.pkl Training time : 88.41\n",
    "Trained Model : xgb_ovr_100_1.pkl Training time : 205.23\n",
    "Shape of the dataset =  (35837, 9)\n",
    "Number of Unique set of ICDs =  19547\n",
    "Trained Model : lr_ovr_100_0.pkl Training time : 8.06\n",
    "Trained Model : lr_ovr_100_1.pkl Training time : 24.45\n",
    "Trained Model : svm_ovr_100_0.pkl Training time : 2.74\n",
    "Trained Model : svm_ovr_100_1.pkl Training time : 147.86\n",
    "Trained Model : xgb_ovr_100_0.pkl Training time : 90.81\n",
    "Trained Model : xgb_ovr_100_1.pkl Training time : 213.17\n",
    "Shape of the dataset =  (36975, 9)\n",
    "Number of Unique set of ICDs =  28172\n",
    "Trained Model : lr_ovr_250_0.pkl Training time : 18.89\n",
    "Trained Model : lr_ovr_250_1.pkl Training time : 56.17\n",
    "Trained Model : svm_ovr_250_0.pkl Training time : 8.03\n",
    "Trained Model : svm_ovr_250_1.pkl Training time : 225.57\n",
    "Trained Model : xgb_ovr_250_0.pkl Training time : 233.72\n",
    "Trained Model : xgb_ovr_250_1.pkl Training time : 494.59\n",
    "Shape of the dataset =  (36975, 9)\n",
    "Number of Unique set of ICDs =  28172\n",
    "Trained Model : lr_ovr_250_0.pkl Training time : 17.82\n",
    "Trained Model : lr_ovr_250_1.pkl Training time : 48.28\n",
    "Trained Model : svm_ovr_250_0.pkl Training time : 7.24\n",
    "Trained Model : svm_ovr_250_1.pkl Training time : 213.72\n",
    "Trained Model : xgb_ovr_250_0.pkl Training time : 305.20\n",
    "Trained Model : xgb_ovr_250_1.pkl Training time : 481.65\n",
    "Shape of the dataset =  (36975, 9)\n",
    "Number of Unique set of ICDs =  28172\n",
    "Trained Model : lr_ovr_250_0.pkl Training time : 21.68\n",
    "Trained Model : lr_ovr_250_1.pkl Training time : 85.77\n",
    "Trained Model : svm_ovr_250_0.pkl Training time : 10.16\n",
    "Trained Model : svm_ovr_250_1.pkl Training time : 332.21\n",
    "Trained Model : xgb_ovr_250_0.pkl Training time : 248.13\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Evaluation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>topicvectorused</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>hammingloss</th>\n",
       "      <th>mdlfittime</th>\n",
       "      <th>prici_array</th>\n",
       "      <th>recall_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lr_ovr_50_0.pkl</td>\n",
       "      <td>0</td>\n",
       "      <td>0.359289</td>\n",
       "      <td>0.065277</td>\n",
       "      <td>0.259948</td>\n",
       "      <td>0.613742</td>\n",
       "      <td>0.101653</td>\n",
       "      <td>4.66</td>\n",
       "      <td>[0.11829895481944368, 0.7062208824335434, 1.0]</td>\n",
       "      <td>[1.0, 0.24093118922961854, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lr_ovr_50_1.pkl</td>\n",
       "      <td>1</td>\n",
       "      <td>0.360493</td>\n",
       "      <td>0.064845</td>\n",
       "      <td>0.260323</td>\n",
       "      <td>0.614288</td>\n",
       "      <td>0.101653</td>\n",
       "      <td>12.35</td>\n",
       "      <td>[0.11829895481944368, 0.7047062023939065, 1.0]</td>\n",
       "      <td>[1.0, 0.24219334330590875, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lr_ovr_50_0.pkl</td>\n",
       "      <td>0</td>\n",
       "      <td>0.359289</td>\n",
       "      <td>0.065277</td>\n",
       "      <td>0.259948</td>\n",
       "      <td>0.613742</td>\n",
       "      <td>0.101653</td>\n",
       "      <td>5.25</td>\n",
       "      <td>[0.11829895481944368, 0.7062208824335434, 1.0]</td>\n",
       "      <td>[1.0, 0.24093118922961854, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lr_ovr_50_1.pkl</td>\n",
       "      <td>1</td>\n",
       "      <td>0.360493</td>\n",
       "      <td>0.064845</td>\n",
       "      <td>0.260323</td>\n",
       "      <td>0.614288</td>\n",
       "      <td>0.101653</td>\n",
       "      <td>11.74</td>\n",
       "      <td>[0.11829895481944368, 0.7047062023939065, 1.0]</td>\n",
       "      <td>[1.0, 0.24219334330590875, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lr_ovr_50_0.pkl</td>\n",
       "      <td>0</td>\n",
       "      <td>0.359289</td>\n",
       "      <td>0.065277</td>\n",
       "      <td>0.259948</td>\n",
       "      <td>0.613742</td>\n",
       "      <td>0.101653</td>\n",
       "      <td>4.21</td>\n",
       "      <td>[0.11829895481944368, 0.7062208824335434, 1.0]</td>\n",
       "      <td>[1.0, 0.24093118922961854, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>xgb_ovr_250_0.pkl</td>\n",
       "      <td>0</td>\n",
       "      <td>0.313415</td>\n",
       "      <td>0.025828</td>\n",
       "      <td>0.172133</td>\n",
       "      <td>0.599202</td>\n",
       "      <td>0.034023</td>\n",
       "      <td>248.13</td>\n",
       "      <td>[0.03847034333690033, 0.7006436737438755, 1.0]</td>\n",
       "      <td>[1.0, 0.20185441461389428, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>xgb_ovr_250_1.pkl</td>\n",
       "      <td>1</td>\n",
       "      <td>0.313678</td>\n",
       "      <td>0.024476</td>\n",
       "      <td>0.172127</td>\n",
       "      <td>0.599343</td>\n",
       "      <td>0.034032</td>\n",
       "      <td>495.99</td>\n",
       "      <td>[0.03847034333690033, 0.6996168582375479, 1.0]</td>\n",
       "      <td>[1.0, 0.20215887074453362, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>lr_ovr_250_0.pkl</td>\n",
       "      <td>0</td>\n",
       "      <td>0.269046</td>\n",
       "      <td>0.019878</td>\n",
       "      <td>0.149422</td>\n",
       "      <td>0.581697</td>\n",
       "      <td>0.034633</td>\n",
       "      <td>23.56</td>\n",
       "      <td>[0.038362800998759536, 0.7068130830086197, 1.0]</td>\n",
       "      <td>[1.0, 0.16614393960420773, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>lr_ovr_250_1.pkl</td>\n",
       "      <td>1</td>\n",
       "      <td>0.269548</td>\n",
       "      <td>0.019743</td>\n",
       "      <td>0.149665</td>\n",
       "      <td>0.581888</td>\n",
       "      <td>0.034625</td>\n",
       "      <td>48.32</td>\n",
       "      <td>[0.038362800998759536, 0.7067137809187279, 1.0]</td>\n",
       "      <td>[1.0, 0.16653251547364623, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>svm_ovr_250_0.pkl</td>\n",
       "      <td>0</td>\n",
       "      <td>0.291102</td>\n",
       "      <td>0.021366</td>\n",
       "      <td>0.153586</td>\n",
       "      <td>0.591641</td>\n",
       "      <td>0.034984</td>\n",
       "      <td>9.62</td>\n",
       "      <td>[0.038362800998759536, 0.6537455179765481, 1.0]</td>\n",
       "      <td>[1.0, 0.1872380582308696, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name topicvectorused        f1  accuracy  avg_precision  \\\n",
       "0     lr_ovr_50_0.pkl               0  0.359289  0.065277       0.259948   \n",
       "1     lr_ovr_50_1.pkl               1  0.360493  0.064845       0.260323   \n",
       "2     lr_ovr_50_0.pkl               0  0.359289  0.065277       0.259948   \n",
       "3     lr_ovr_50_1.pkl               1  0.360493  0.064845       0.260323   \n",
       "4     lr_ovr_50_0.pkl               0  0.359289  0.065277       0.259948   \n",
       "..                ...             ...       ...       ...            ...   \n",
       "81  xgb_ovr_250_0.pkl               0  0.313415  0.025828       0.172133   \n",
       "82  xgb_ovr_250_1.pkl               1  0.313678  0.024476       0.172127   \n",
       "83   lr_ovr_250_0.pkl               0  0.269046  0.019878       0.149422   \n",
       "84   lr_ovr_250_1.pkl               1  0.269548  0.019743       0.149665   \n",
       "85  svm_ovr_250_0.pkl               0  0.291102  0.021366       0.153586   \n",
       "\n",
       "     roc_auc  hammingloss mdlfittime  \\\n",
       "0   0.613742     0.101653       4.66   \n",
       "1   0.614288     0.101653      12.35   \n",
       "2   0.613742     0.101653       5.25   \n",
       "3   0.614288     0.101653      11.74   \n",
       "4   0.613742     0.101653       4.21   \n",
       "..       ...          ...        ...   \n",
       "81  0.599202     0.034023     248.13   \n",
       "82  0.599343     0.034032     495.99   \n",
       "83  0.581697     0.034633      23.56   \n",
       "84  0.581888     0.034625      48.32   \n",
       "85  0.591641     0.034984       9.62   \n",
       "\n",
       "                                        prici_array  \\\n",
       "0    [0.11829895481944368, 0.7062208824335434, 1.0]   \n",
       "1    [0.11829895481944368, 0.7047062023939065, 1.0]   \n",
       "2    [0.11829895481944368, 0.7062208824335434, 1.0]   \n",
       "3    [0.11829895481944368, 0.7047062023939065, 1.0]   \n",
       "4    [0.11829895481944368, 0.7062208824335434, 1.0]   \n",
       "..                                              ...   \n",
       "81   [0.03847034333690033, 0.7006436737438755, 1.0]   \n",
       "82   [0.03847034333690033, 0.6996168582375479, 1.0]   \n",
       "83  [0.038362800998759536, 0.7068130830086197, 1.0]   \n",
       "84  [0.038362800998759536, 0.7067137809187279, 1.0]   \n",
       "85  [0.038362800998759536, 0.6537455179765481, 1.0]   \n",
       "\n",
       "                       recall_array  \n",
       "0   [1.0, 0.24093118922961854, 0.0]  \n",
       "1   [1.0, 0.24219334330590875, 0.0]  \n",
       "2   [1.0, 0.24093118922961854, 0.0]  \n",
       "3   [1.0, 0.24219334330590875, 0.0]  \n",
       "4   [1.0, 0.24093118922961854, 0.0]  \n",
       "..                              ...  \n",
       "81  [1.0, 0.20185441461389428, 0.0]  \n",
       "82  [1.0, 0.20215887074453362, 0.0]  \n",
       "83  [1.0, 0.16614393960420773, 0.0]  \n",
       "84  [1.0, 0.16653251547364623, 0.0]  \n",
       "85   [1.0, 0.1872380582308696, 0.0]  \n",
       "\n",
       "[86 rows x 10 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdlresults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate cross validatation results of trained models\n",
    "avg_mdlresults_df = pd.read_pickle(\"avg_mdlresults_df\")\n",
    "avg_mdlresults_df = avg_mdlresults_df.sort_values('name')\n",
    "avg_mdlresults_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model from the Models directory :\n",
    "# We have trained models for 3 different classifiers namely LogisticRegression (lr), SVM (svm) and XGBoost (xgb)\n",
    "# Models are trained for predicting 50, 100 and 250 most commonly used ICDs, and the number will be reflected in the name\n",
    "# Please select appropriate value of variable \"mostcommon\" based on most common ICDs to predict i.e. 50, 100 or 250\n",
    "\n",
    "id=999\n",
    "num = 20 # Number of topics from topic vector obtained from topic modeling \n",
    "X_train, X_test, y_train, y_test = dataprep_topmdlfeat(df, diagicd_df, mostcommon=250, numtopics=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl1 = joblib.load('Models/lr_ovr_250_1.pkl')\n",
    "humaneval(y_test,mdl1.predict(X_test),id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl1 = joblib.load('Models/xgb_ovr_250_1.pkl')\n",
    "humaneval(y_test,mdl1.predict(X_test),id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved example of human evaluation of model performance and discussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human assigned ICDs -  ('272.4', '285.1', '287.5', '424.1')\n",
      "Model assigned ICDs -  ('272.4', '401.9', '424.1')\n",
      "Correctly assigned ICDs -  ['272.4', '424.1']\n",
      "Features extracted from Discharge Diagnosis:  ['coronary', 'artery', 'disease', 'gerd', 'hyperlipidemia', 'aortic', 'stenosis', 'obesity', 'cataracts']\n"
     ]
    }
   ],
   "source": [
    "# Model - Logistic Regression, One Vs Rest, 200 most common ICDs\n",
    "# id=8\n",
    "# humaneval(y_test,y_pred,id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the human encoder has assigned ICDs -\n",
    "- '272.4' = Other and unspecified hyperlipidemia\n",
    "- '285.1' = Acute posthemorrhagic anemia\n",
    "- '287.5' = Thrombocytopenia, unspecified\n",
    "- '424.1' = Aortic valve disorders\n",
    "\n",
    "Features extracted from Discharge Diagnosis:  ['coronary', 'artery', 'disease', 'gerd', 'hyperlipidemia', 'aortic', 'stenosis', 'obesity', 'cataracts']\n",
    "\n",
    "It seems that our model was able to correctly pick codes based on relevant words like 'coronary', 'artery', 'disease','hyperlipidemia', 'aortic', 'stenosis' but failed to pick Thrombocytopenia which is a condition in which you have a low blood platelet count, and Acute posthemorrhagic anemia is a condition that develops when you lose a large amount of blood quickly. The failure for picking these could be blamed on not extracting all the relevant information from the NOTESEVENTS.csv which contains the full Electronic Health Record of the patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human assigned ICDs -  ('272.0', 'V17.3')\n",
      "Model assigned ICDs -  ('401.9', '424.0')\n",
      "Correctly assigned ICDs -  []\n",
      "Features extracted from Discharge Diagnosis:  ['mitral', 'valve', 'coronary', 'artery']\n"
     ]
    }
   ],
   "source": [
    "# # Model - Logistic Regression, One Vs Rest, 200 most common ICDs\n",
    "# id=555\n",
    "# humaneval(y_test,y_pred,id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the human encoder has assigned ICDs -\n",
    "- '272.4' = Pure hypercholesterolemia \n",
    "- 'V17.3' = Family history of ischemic heart disease\n",
    "\n",
    "It seems our model failed to pick heart related terms such as 'valve', 'coronary', 'artery', but the context here is family history of heart dieseases and that was not present in the portion of the text that was extracted for the sample. We believe that incorporating more relevant sections of the discharge note might improve the model prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for future work:\n",
    "\n",
    "For unsupervised learning supplement the unsupervised dataset with long description of code as EHR diagnosis note and pass the single ICD code as a label, to gauge how well the ICD code is attributed to different topics identified by the model. ICD-9 CM codes are broadly categorized in 20 categories. We can compare if the model congregates related codes in correct clusters or not. Improved topic models can lead to better topic vectors which have already shown to help improve the performance of the supervised learning model.\n",
    "\n",
    "Explore use of Top2vec model for topic modeling task.\n",
    "\n",
    "Filtering the entire note based on the SNOMED-CT medical terms dictionary.\n",
    "\n",
    "Considering extracting bi-grams and ni-grams as features.\n",
    "\n",
    "Add code recommendation explainability which highlights which parts of the diagnosis note are relevant for the recommended code\n",
    "\n",
    "We are currently predicting codes in the format \"xyz.a\" (complete format is xyz.abc). We will test the model prediction for codes in the format \"xyz\"   i.e. at the category level, and in the format \"xyz.ab\" and \"xyz.abc\"\n",
    "\n",
    "State of the art current research on auto labeling of ICDs show that deep neural networks with attention have significantly improved the prediction performance. We will train a LSTM model and explore scope for further improvement using deep learning methods. \n",
    "\n",
    "Explore if the learning from this project/model be used to improve/implement template for free text inputs from human operators.\n",
    "\n",
    "Auto complete suggestions to human operators while they input patient notes in the system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethics Discussion: \n",
    "\n",
    "- Since the ICD codes are used for insurance billing and reimbursement purposes, incorrect ICD if used as wrongly suggested by the model then it may cause monetary harm to the individuals or institutions.\n",
    "- Incorrect labeling of ICDs may harm any scientific research, clinical trials done based on such records.\n",
    "- General rule among human medical codes is that the most appropriate ICD among set of relevant ICDs is to be used; the model in current state lacks the awareness to assess that, and we as the model developers lack the domain expertise to make that judgment. \n",
    "- For insurance purposes, some codes may result in higher insurance payouts than other comparable codes, which may introduce bias towards such predictions from the model if it is present in the underlying data. We have to be cognizant of this with our model predictions.\n",
    "\n",
    "Hence the current model is prone to making mistakes and it should not be deployed as it is and more work is needed given the potential harm listed above. We believe that significant human in the loop validation work is required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22344b379ba690b458dd29ec51967cd08c9ef4cfa52568c0aac6d5eb8e286ea5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
